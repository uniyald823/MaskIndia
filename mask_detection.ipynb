{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "data_path=\"C:/Users/Drishya/Desktop/Dataset/train\"\n",
    "categories=os.listdir(data_path)\n",
    "labels=[i for i in range(len(categories))]\n",
    "\n",
    "label_dict=dict(zip(categories,labels))\n",
    "\n",
    "data=[]\n",
    "target=[]\n",
    "\n",
    "for category in categories:\n",
    "    folder_path=os.path.join(data_path,category)\n",
    "    img_names=os.listdir(folder_path)\n",
    "        \n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "\n",
    "        try:\n",
    "            gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) \n",
    "            resized=cv2.resize(gray,(100, 100))\n",
    "            data.append(resized)\n",
    "            target.append(label_dict[category])\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Exception:',e)\n",
    "            \n",
    "\n",
    "data=np.array(data)/255.0\n",
    "data=np.reshape(data,(data.shape[0],100, 100,1))\n",
    "target=np.array(target)\n",
    "\n",
    "\n",
    "\n",
    "new_target=np_utils.to_categorical(target)\n",
    "\n",
    "np.save('data',data)\n",
    "np.save('target',new_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.5476WARNING:tensorflow:From C:\\Users\\Drishya\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Drishya\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "30/30 [==============================] - 82s 3s/step - loss: 0.6943 - accuracy: 0.5476 - val_loss: 0.6189 - val_accuracy: 0.7089\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.7844INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "30/30 [==============================] - 78s 3s/step - loss: 0.5038 - accuracy: 0.7844 - val_loss: 0.4041 - val_accuracy: 0.8312\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3376 - accuracy: 0.8658INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "30/30 [==============================] - 77s 3s/step - loss: 0.3376 - accuracy: 0.8658 - val_loss: 0.3842 - val_accuracy: 0.8481\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.9027INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "30/30 [==============================] - 70s 2s/step - loss: 0.2556 - accuracy: 0.9027 - val_loss: 0.2889 - val_accuracy: 0.8945\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 56s 2s/step - loss: 0.2044 - accuracy: 0.9228 - val_loss: 0.3896 - val_accuracy: 0.8354\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9345INFO:tensorflow:Assets written to: model-006.model\\assets\n",
      "30/30 [==============================] - 72s 2s/step - loss: 0.1630 - accuracy: 0.9345 - val_loss: 0.2309 - val_accuracy: 0.9241\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 61s 2s/step - loss: 0.1422 - accuracy: 0.9503 - val_loss: 0.3653 - val_accuracy: 0.8439\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 61s 2s/step - loss: 0.1312 - accuracy: 0.9493 - val_loss: 0.2701 - val_accuracy: 0.8903\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9662INFO:tensorflow:Assets written to: model-009.model\\assets\n",
      "30/30 [==============================] - 80s 3s/step - loss: 0.1026 - accuracy: 0.9662 - val_loss: 0.2048 - val_accuracy: 0.9241\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 61s 2s/step - loss: 0.0880 - accuracy: 0.9736 - val_loss: 0.2709 - val_accuracy: 0.8987\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 61s 2s/step - loss: 0.0486 - accuracy: 0.9905 - val_loss: 0.2162 - val_accuracy: 0.9241\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 60s 2s/step - loss: 0.0427 - accuracy: 0.9820 - val_loss: 0.2130 - val_accuracy: 0.9325\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 59s 2s/step - loss: 0.0565 - accuracy: 0.9778 - val_loss: 0.2554 - val_accuracy: 0.9072\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 59s 2s/step - loss: 0.0493 - accuracy: 0.9799 - val_loss: 0.3404 - val_accuracy: 0.8903\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 59s 2s/step - loss: 0.0389 - accuracy: 0.9894 - val_loss: 0.2396 - val_accuracy: 0.9367\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 59s 2s/step - loss: 0.0288 - accuracy: 0.9873 - val_loss: 0.2142 - val_accuracy: 0.9283\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 61s 2s/step - loss: 0.0273 - accuracy: 0.9905 - val_loss: 0.2225 - val_accuracy: 0.9072\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 60s 2s/step - loss: 0.0226 - accuracy: 0.9937 - val_loss: 0.2710 - val_accuracy: 0.9283\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 56s 2s/step - loss: 0.0398 - accuracy: 0.9873 - val_loss: 0.2608 - val_accuracy: 0.9114\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 61s 2s/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 0.2963 - val_accuracy: 0.8945\n",
      "5/5 [==============================] - 2s 358ms/step - loss: 0.2881 - accuracy: 0.9091\n",
      "[0.28812748193740845, 0.9090909361839294]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential #keras is a deep learning API\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data=np.load('data.npy')\n",
    "target=np.load('target.npy')\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(100,(3,3),input_shape=data.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(100,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten()) #converts data to 1-d array\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "train_data,test_data,train_target,test_target=train_test_split(data,target,test_size=0.1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "  'model-{epoch:03d}.model',\n",
    "  monitor='val_loss',\n",
    "  verbose=0,\n",
    "  save_best_only=True,\n",
    "  mode='auto')\n",
    "\n",
    "history=model.fit(\n",
    "  train_data,\n",
    "  train_target,\n",
    "  epochs=20,\n",
    "  callbacks=[checkpoint],\n",
    "  validation_split=0.2)\n",
    "\n",
    "print(model.evaluate(test_data,test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mask_recog_ver2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-95673613251b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     faces = faceCascade.detectMultiScale(gray,\n\u001b[0m\u001b[0;32m     19\u001b[0m                                          \u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                          \u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_modelq\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    " \n",
    "cascPath = os.path.dirname(\n",
    "    cv2.__file__) + \"C:/Users/Drishya/OneDrive/Desktop/major_project/haarcascade_frontalface_alt2.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "model = load_model(\"mask_recog_ver2.h5\")\n",
    " \n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    faces_list=[]\n",
    "    preds=[]\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_frame = frame[y:y+h,x:x+w]\n",
    "        face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2RGB)\n",
    "        face_frame = cv2.resize(face_frame, (224, 224))\n",
    "        face_frame = img_to_array(face_frame)\n",
    "        face_frame = np.expand_dims(face_frame, axis=0)\n",
    "        face_frame =  preprocess_input(face_frame)\n",
    "        faces_list.append(face_frame)\n",
    "        if len(faces_list)>0:\n",
    "            preds = model.predict(faces_list)\n",
    "        for pred in preds:\n",
    "            (mask, withoutMask) = pred\n",
    "        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "        cv2.putText(frame, label, (x, y- 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    " \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h),color, 2)\n",
    "        # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "'''for (box, pred) in zip(locs, preds):\n",
    "        # unpack the bounding box and predictions\n",
    "        (startX, startY, endX, endY) = box\n",
    "        (mask, withoutMask,notproper) = pred\n",
    "        # determine the class label and color we'll use to draw\n",
    "        # the bounding box and text\n",
    "        if (mask > withoutMask and mask>notproper):\n",
    "            label = \"Without Mask\"\n",
    "        elif ( withoutMask > notproper and withoutMask > mask):\n",
    "            label = \"Mask\"\n",
    "        else:\n",
    "            label = \"Wear Mask Properly\"\n",
    "\n",
    "        if label == \"Mask\":\n",
    "            color = (0, 255, 0)\n",
    "        elif label==\"Without Mask\":\n",
    "            color = (0, 0, 255)\n",
    "        else:\n",
    "            color = (255, 140, 0)\n",
    "\n",
    "        # include the probability in the label\n",
    "        label = \"{}: {:.2f}%\".format(label,\n",
    "                                     max(mask, withoutMask, notproper) * 100)\n",
    "\n",
    "        # display the label and bounding box rectangle on the output\n",
    "        # frame\n",
    "cv2.putText(frame, label, (startX, startY - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)'''\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
